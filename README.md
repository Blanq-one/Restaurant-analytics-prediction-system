# Restaurant Data Management and Analysis System

A centralized analytics platform that stores normalized restaurant transactional data, computes business KPIs, and provides demand prediction/forecasting -- built with Python, SQL, and time-series ML models.

## Problem Statement

Small and mid-sized restaurants generate large volumes of daily data (orders, payments, menu items, customer visits) but often lack a centralized system to track performance. This project solves that by building an end-to-end data pipeline that:

- Stores normalized data in a relational database (3NF)
- Computes 10+ business KPIs automatically
- Provides time-series sales analysis
- Predicts future revenue using SARIMA and Prophet models
- Exports BI-ready dashboards and Excel reports

## Key Features

| Feature | Description |
|---|---|
| Normalized Schema | 3NF database with referential integrity, indexes, and constraints |
| ETL Pipeline | CSV ingestion with validation, deduplication, and timestamp normalization |
| 10 Business KPIs | Daily/weekly/monthly revenue, AOV, peak hours, category analysis, and more |
| Demand Prediction | SARIMA + Prophet models with train/test evaluation (MAE, RMSE, MAPE) |
| SQL Analytics | KPI views, time-series grouping, window functions |
| Excel Export | Auto-generated KPI workbook with multiple sheets |
| Unit Tests | 43 pytest tests covering validators, transformers, and KPI calculators |
| SOLID Design | Protocol-based abstractions, single-responsibility modules, extensible KPIs |

## Tech Stack

- **Python** -- Pandas, SQLAlchemy, Prophet, Statsmodels
- **SQL** -- PostgreSQL / SQLite (schema, views, indexes)
- **ML** -- SARIMA, Prophet (time-series forecasting)
- **Testing** -- pytest (43 tests)
- **Visualization** -- Matplotlib, Jupyter Notebooks
- **Data** -- Kaggle restaurant dataset (5,000 orders, 12,895 line items)

## Project Structure

```
restaurant-analytics-system/
|
|-- data/
|   |-- raw/                  # Source CSVs (generated by pipeline)
|   |-- staging/              # Enriched order detail table
|   |-- warehouse/            # KPI outputs, Excel report, SQLite DB
|
|-- sql/
|   |-- schema/               # CREATE TABLE statements (PostgreSQL + SQLite)
|   |-- views/                # KPI views, sales trend views
|   |-- indexes.sql           # Performance indexes
|
|-- src/
|   |-- config/
|   |   |-- db_config.py      # Database connection (env-based)
|   |
|   |-- models/               # Data classes (Customer, Order, MenuItem)
|   |
|   |-- services/             # Core business logic
|   |   |-- data_loader.py    # Repository pattern for DB access
|   |   |-- validator.py      # Data validation (null checks, required cols)
|   |   |-- transformer.py    # Timestamp normalization, deduplication
|   |   |-- kpi_calculator.py # 10 KPI classes (SOLID, protocol-based)
|   |   |-- sample_data_generator.py
|   |
|   |-- views/                # Output layer
|   |   |-- sql_views.py      # Apply SQL views to database
|   |   |-- export_excel.py   # Multi-sheet Excel export
|   |
|   |-- pipeline.py           # Main ETL orchestrator
|
|-- tests/                    # 43 unit tests
|   |-- test_validator.py
|   |-- test_transformer.py
|   |-- test_kpi_calculator.py
|   |-- test_export_excel.py
|
|-- notebooks/
|   |-- 01_data_validation.ipynb
|   |-- 02_sql_analysis.ipynb
|   |-- 03_dashboard_metrics.ipynb
|   |-- 04_prediction_forecasting.ipynb
|
|-- airflow/
|   |-- daily_etl_dag.py      # Optional Airflow DAG
|
|-- requirements.txt
|-- README.md
```

## Quick Start

### 1. Clone and install

```bash
git clone https://github.com/<your-username>/restaurant-analytics-system.git
cd restaurant-analytics-system
pip install -r requirements.txt
```

### 2. Generate sample data

```bash
python -m src.services.sample_data_generator
```

This creates 5,000 orders with 12,895 line items across 45 menu items and 6 categories in `data/raw/`.

### 3. Run the pipeline

```bash
python -m src.pipeline
```

This will:
- Validate and transform raw CSVs
- Compute 10 KPI tables
- Export `data/warehouse/kpi_report.xlsx`
- Load everything into a local SQLite database

### 4. Run tests

```bash
python -m pytest tests/ -v
```

### 5. Open notebooks

```bash
jupyter notebook notebooks/
```

## Business KPIs Computed

| KPI | Description |
|---|---|
| Daily Revenue | Revenue aggregated by date |
| Weekly Revenue | Revenue grouped by ISO week |
| Monthly Revenue | Revenue grouped by year-month |
| Average Order Value | Mean revenue per order |
| Orders Per Day | Unique order count by date |
| Revenue Per Hour | Hourly revenue distribution |
| Peak Hours | Busiest hours ranked by order count |
| Weekday vs Weekend | Revenue and order split |
| Top Menu Items | Items ranked by revenue and quantity |
| Revenue by Category | Category-level revenue breakdown |

## Prediction / Forecasting

The `04_prediction_forecasting.ipynb` notebook implements three models:

| Model | Approach |
|---|---|
| Baseline | 7-day moving average |
| SARIMA(1,1,1)(1,1,0,7) | Seasonal ARIMA with weekly seasonality |
| Prophet | Facebook's additive model with trend + seasonality |

Models are evaluated on a 30-day held-out test set using MAE, RMSE, and MAPE. The best model then generates a 7-day revenue prediction with confidence intervals.

## Database Configuration

By default, the pipeline uses a local SQLite database (`data/warehouse/restaurant.db`). To use PostgreSQL instead, set the environment variable:

```bash
export DATABASE_URL="postgresql+psycopg2://user:pass@host:5432/dbname"
```

## SOLID Principles

- **Single Responsibility** -- Each module has one job (validator, transformer, KPI calculator)
- **Open/Closed** -- New KPIs extend `KPIBase` protocol without modifying existing code
- **Liskov Substitution** -- Any `KPIBase` implementation works with `run_kpi()`
- **Interface Segregation** -- Separate protocols for `DataRepository`, `Validator`, `KPIBase`
- **Dependency Inversion** -- Pipeline depends on abstractions (`DataRepository`), not concrete DB engines

## Running Tests

```bash
# All tests
python -m pytest tests/ -v

# Specific module
python -m pytest tests/test_kpi_calculator.py -v
```

## Dataset

Sample data is generated using `src/services/sample_data_generator.py`, inspired by:
- [Restaurant Sales Report](https://www.kaggle.com/datasets/rajatsurana979/restaurant-sales-report) (Kaggle)
- [Coffee Shop Sales](https://www.kaggle.com/datasets/dieterholger/coffee-shop-sales) (Kaggle)

## License

MIT
